{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dotto\\anaconda3\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dotto\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "# Load train_data and plug into Huggingface libraries\n",
    "# Casual Language Modeling using distillgpt2\n",
    "# 1. load texts and concatenate them after tokenization.\n",
    "# 2. split them in examples of sequence length\n",
    "\n",
    "\n",
    "# install transformers\n",
    "#! pip install transformers\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count                                                  8034\n",
      "unique                                                 8034\n",
      "top       agent: Hi! agent: How can I help you? customer...\n",
      "freq                                                      1\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    agent: Hi! agent: How can I help you? customer...\n",
       "1    agent: good afternoon, how can I help you? cus...\n",
       "2    customer: HEY HO! agent: good afternoon, how c...\n",
       "3    agent: Welcome to AcmeBrands! How can I help y...\n",
       "4    agent: Hello, how can i help you customer: Hel...\n",
       "5    agent: hello! How can I help you today? custom...\n",
       "6    agent: Hello how can I help you today? custome...\n",
       "7    customer: Hello agent: Hello! How can I help y...\n",
       "8    agent: Thank you for contacting Acme Brands! H...\n",
       "9    agent: Thank you for contacting Acme.  How may...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load train_data as datasets\n",
    "# read text file put in DataFrame\n",
    "datasets = pd.read_fwf('data/train_data.txt', header = None)\n",
    "df_train = pd.DataFrame(datasets)\n",
    "\n",
    "# drop all except conversations\n",
    "# save convo by row \n",
    "# row1 - convo 1\n",
    "# row2 - convo 2\n",
    "df_train = df_train.iloc[:,0]\n",
    "\n",
    "\n",
    "# sneak peak\n",
    "print(df_train.describe())\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLM _ Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1789743 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13384/2010601240.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m dataset = TextDataset(\n\u001b[0;32m     25\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mtext_column\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# tokenized input_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mblock_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m  \u001b[1;31m#adjust block size after discussion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "# load distilgpt2 model\n",
    "model_name = 'distilgpt2'\n",
    "\n",
    "# load tokenizer\n",
    "# to tokenize all texts with same vocabs\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# After cleaning data, instantiate Trainer\n",
    "# Use model distilGPT2\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# prepare dataset\n",
    "\n",
    "# put train data into list\n",
    "texts = df_train.tolist()\n",
    "text_data = \" \".join(texts) #!\n",
    "# tokenize concatenated text data\n",
    "input_ids = tokenizer(text_data).input_ids\n",
    "\n",
    "# create dataset\n",
    "from transformers import TextDataset\n",
    "dataset = TextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    text_column = input_ids.tolist(), # tokenized input_ids #!\n",
    "    block_size = 128  #adjust block size after discussion\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer instance \n",
    "'''\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model}-Finetuned-train-data\",\n",
    "    evaluation_strategy='epoch',\n",
    "    laerning_rate=2e-5, #?\n",
    "    weight_decay= .01, #TBD\n",
    "    push_to_hub=True,\n",
    "    \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "# Train data\n",
    "trainer.train()\n",
    "'''\n",
    "\n",
    "# Don't run this code rn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
